<<<<<<< HEAD:scripts/crawling.policy.properties
reinitCounts=true
=======
include = /data1/wdl.properties/pulsar.bubing.dev80.properties
rootDir=/data1/crawling/data
maxInstantSchemeAuthorityPerIP=256
maxVisitStates=30000000
parsingThreads=3
dnsThreads=800
dnsTimeout=12s
dnsCacheMaxSize=4Mi
dnsPositiveTtl=10h
dnsNegativeTtl=10m
fetchingThreads=2400
>>>>>>> 35acc3625e1ef17ede5e82abe943fe025041f62e:scripts/crawl.4cores.properties
fetchFilter=( SchemeEquals(http) or SchemeEquals(https) ) and not PathEndsWithOneOf(.axd\,.xls\,.rar\,.sflb\,.tmb\,.pdf\,.js\,.swf\,.rss\,.kml\,.m4v\,.tif\,.avi\,.iso\,.mov\,.ppt\,.bib\,.docx\,.css\,.fits\,.png\,.gif\,.jpg\,.jpeg\,.ico\,.doc\,.wmv\,.mp3\,.mp4\,.aac\,.ogg\,.wma\,.gz\,.bz2\,.Z\,.z\,.zip) and URLShorterThan(512) and DuplicateSegmentsLessThan(3) and not URLMatchesRegex(.*\\?zenid=.*|.*submit\\?.*|.*Products\\.asp\\?.*|.*cart\\?add.*)
scheduleFilter=( SchemeEquals(http) or SchemeEquals(https) ) and not PathEndsWithOneOf(.axd\,.xls\,.rar\,.sflb\,.tmb\,.pdf\,.js\,.swf\,.rss\,.kml\,.m4v\,.tif\,.avi\,.iso\,.mov\,.ppt\,.bib\,.docx\,.css\,.fits\,.png\,.gif\,.jpg\,.jpeg\,.ico\,.doc\,.wmv\,.mp3\,.mp4\,.aac\,.ogg\,.wma\,.gz\,.bz2\,.Z\,.z\,.zip) and URLShorterThan(512) and DuplicateSegmentsLessThan(3) and not URLMatchesRegex(.*\\?zenid=.*|.*submit\\?.*|.*Products\\.asp\\?.*|.*cart\\?add.*)
followFilter=true
parseFilter=StatusCategory(3) or ( ContentTypeStartsWith(text/) or PathEndsWithOneOf(.html\,.htm\,.txt) ) and not IsProbablyBinary()
storeFilter=( ContentTypeStartsWith(text/) or PathEndsWithOneOf(.html\,.htm\,.txt) ) and not IsProbablyBinary()
<<<<<<< HEAD:scripts/crawling.policy.properties
bloomFilterPrecision=1E-6
seed=http://www.exensa.com/
=======
schemeAuthorityDelay=8s
ipDelay=4s
socketTimeout=5s
connectionTimeout=8s
maximumTimeToFirstByte=6000
fetchDataBufferByteSize=200K
>>>>>>> 35acc3625e1ef17ede5e82abe943fe025041f62e:scripts/crawl.4cores.properties
cookiePolicy=standard
userAgent=Barkrowler/0.9 (+https://babbar.tech/crawler)
userAgentFrom=tech@babbar.tech
robotsExpiration=100h
robotProxyHost=localhost
robotProxyPort=3128
digestAlgorithm=MurmurHash3
startPaused=false
<<<<<<< HEAD:scripts/crawling.policy.properties
=======
workbenchMaxByteSize=2Gi
>>>>>>> 35acc3625e1ef17ede5e82abe943fe025041f62e:scripts/crawl.4cores.properties
parserSpec=XHTMLParser(MurmurHash3)
storeClass=it.unimi.di.law.bubing.store.PulsarWarcStore
blackListedIPv4Addresses=file:blacklist.txt
